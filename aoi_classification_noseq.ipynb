{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aoi classification  0-pass ，1-defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['defect', 'defect_clean', 'pass', 'test', 'test01', 'test_pass', 'train', 'train01']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import random\n",
    "import os,shutil\n",
    "\n",
    "src_path=\"..\\\\tensorflow-classification\\\\dataset\"\n",
    "\n",
    "print(os.listdir(src_path))\n",
    "\n",
    "#constant value\n",
    "VALID_SPIT=0.2\n",
    "IMAGE_SIZE=80\n",
    "BATCH_SIZE=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 80, 80, 3)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "label=[]\n",
    "data=[]\n",
    "counter=0\n",
    "path=\"..\\\\tensorflow-classification\\\\dataset\\\\train01\"\n",
    "for file in os.listdir(path):\n",
    "    image_data=cv2.imread(os.path.join(path,file), cv2.IMREAD_COLOR)\n",
    "    image_data=cv2.resize(image_data,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "    if file.startswith(\"pass\"):\n",
    "        label.append(0)\n",
    "    elif file.startswith(\"defect\"):\n",
    "        label.append(1)\n",
    "    try:\n",
    "        data.append(image_data/255)\n",
    "    except:\n",
    "        label=label[:len(label)-1]\n",
    "    counter+=1\n",
    "    if counter%200==0:\n",
    "        print (counter,\" image data retreived\")\n",
    "\n",
    "data=np.array(data)\n",
    "data=data.reshape((data.shape)[0],(data.shape)[1],(data.shape)[2],3)\n",
    "label=np.array(label)\n",
    "print (data.shape)\n",
    "print (label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 80, 80, 3)\n",
      "(144,)\n",
      "(36, 80, 80, 3)\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, valid_data, train_label, valid_label = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_data.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use vgg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nien\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80, 80, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 80, 80, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 40, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 40, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 20, 20, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "vgg_model = applications.VGG16(weights='imagenet',\n",
    "                               include_top=False,\n",
    "                               input_shape=(80, 80, 3))\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_2': <keras.engine.topology.InputLayer object at 0x0000021B32F84BE0>, 'block1_conv1': <keras.layers.convolutional.Conv2D object at 0x0000021B32F84CC0>, 'block1_conv2': <keras.layers.convolutional.Conv2D object at 0x0000021B32F84D68>, 'block1_pool': <keras.layers.pooling.MaxPooling2D object at 0x0000021B32F84E48>, 'block2_conv1': <keras.layers.convolutional.Conv2D object at 0x0000021B32FA03C8>, 'block2_conv2': <keras.layers.convolutional.Conv2D object at 0x0000021B32FA0C18>, 'block2_pool': <keras.layers.pooling.MaxPooling2D object at 0x0000021B198ECC50>, 'block3_conv1': <keras.layers.convolutional.Conv2D object at 0x0000021B198FBA20>, 'block3_conv2': <keras.layers.convolutional.Conv2D object at 0x0000021B1990F390>, 'block3_conv3': <keras.layers.convolutional.Conv2D object at 0x0000021B19925F28>, 'block3_pool': <keras.layers.pooling.MaxPooling2D object at 0x0000021B1993AF98>, 'block4_conv1': <keras.layers.convolutional.Conv2D object at 0x0000021B32F41940>, 'block4_conv2': <keras.layers.convolutional.Conv2D object at 0x0000021B19965860>, 'block4_conv3': <keras.layers.convolutional.Conv2D object at 0x0000021B19977E80>, 'block4_pool': <keras.layers.pooling.MaxPooling2D object at 0x0000021B19988518>, 'block5_conv1': <keras.layers.convolutional.Conv2D object at 0x0000021B199B09E8>, 'block5_conv2': <keras.layers.convolutional.Conv2D object at 0x0000021B199C4EF0>, 'block5_conv3': <keras.layers.convolutional.Conv2D object at 0x0000021B199DAF28>, 'block5_pool': <keras.layers.pooling.MaxPooling2D object at 0x0000021B199F0160>}\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "vgg_model = applications.VGG16(weights='imagenet',\n",
    "                               include_top=False,\n",
    "                               input_shape=(80, 80, 3))\n",
    "layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers])\n",
    "print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nien\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nien\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ou...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers])\n",
    "x = layer_dict['block5_pool'].output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation  = 'relu')(x)\n",
    "y = layers.Dense(1, activation = 'sigmoid',name='out')(x)\n",
    "model = models.Model(input=vgg_model.input, output=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 80, 80, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 80, 80, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 40, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 40, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 20, 20, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 15,239,489\n",
      "Trainable params: 15,239,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2:0\", shape=(?, 80, 80, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(model.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.topology.InputLayer at 0x1a80c0bcac8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('input_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_sess = tf.Session(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf_output = tf_sess.graph.get_tensor_by_name(\"out/Sigmoid:0\")\n",
    "print(tf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "K.set_image_data_format('channels_last')\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.adam(lr=1e-4),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"check\\\\model.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4183 - acc: 0.7986\n",
      "Epoch 2/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 0.1058 - acc: 0.9514\n",
      "Epoch 3/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 0.0564 - acc: 0.9653\n",
      "Epoch 4/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 0.0337 - acc: 0.9931\n",
      "Epoch 5/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 0.0134 - acc: 0.9931\n",
      "Epoch 6/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 9.5362e-04 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 1.0031e-04 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 2.0747e-05 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 1.1994e-05 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 8.0736e-06 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 6.7652e-06 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 5.8119e-06 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 5.1329e-06 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 4.6689e-06 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 4.2531e-06 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 3.8804e-06 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 3.5547e-06 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 3.2585e-06 - acc: 1.0000: 0s - loss: 3.7762e-06 - acc: 1.00\n",
      "Epoch 20/20\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 2.9292e-06 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_history=model.fit(train_data, train_label, epochs=20, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 儲存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model01.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模糊矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(valid_data)\n",
    "predicted_label=np.round(Y_pred,decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Get the confusion matrix\n",
    "\n",
    "CM = confusion_matrix(valid_label, Y_pred.round())\n",
    "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(12, 12))\n",
    "plt.xticks(range(2), ['pass', 'defect'], fontsize=16)\n",
    "plt.yticks(range(2), ['pass', 'defect'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 80, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test_data=[]\n",
    "id=[]\n",
    "counter=0\n",
    "IMAGE_SIZE=80\n",
    "\n",
    "#for file in os.listdir(\"..\\\\tensorflow-classification\\\\dataset\\\\test_pass\"):\n",
    "for file in os.listdir(\"..\\\\tensorflow-classification\\\\dataset\\\\test01\"):\n",
    "    pic = \"..\\\\tensorflow-classification\\\\dataset\\\\test01\\\\\" + file\n",
    "    image_data=cv2.imread(os.path.join(pic), cv2.IMREAD_COLOR)\n",
    "    \n",
    "    try:\n",
    "        image_data=cv2.resize(image_data,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "        test_data.append(image_data/255)\n",
    "        id.append((file.split(\".\"))[0])\n",
    "    except:\n",
    "        print (\"ek gaya\")\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "test_data=np.array(test_data)\n",
    "print (test_data.shape)\n",
    "test_data=test_data.reshape((test_data.shape)[0],(test_data.shape)[1],(test_data.shape)[2],3)\n",
    "dataframe_output=pd.DataFrame({\"id\":id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels=model.predict(test_data)\n",
    "predicted_labels=np.round(predicted_labels,decimals=2)\n",
    "labels=[1 if value>0.5 else 0 for value in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  label\n",
      "0   defect      1\n",
      "1   defect      1\n",
      "2   defect      1\n",
      "3   defect      1\n",
      "4   defect      1\n",
      "5   defect      1\n",
      "6   defect      1\n",
      "7   defect      1\n",
      "8   defect      1\n",
      "9     pass      0\n",
      "10    pass      0\n",
      "11    pass      0\n",
      "12    pass      0\n",
      "13    pass      0\n",
      "14    pass      0\n",
      "15    pass      0\n",
      "16    pass      0\n",
      "17    pass      0\n"
     ]
    }
   ],
   "source": [
    "dataframe_output[\"label\"]=labels\n",
    "print(dataframe_output)\n",
    "dataframe_output.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(40):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(test_data[i])\n",
    "#     plt.xlabel('label')\n",
    "    plt.xlabel(dataframe_output[\"label\"][i],fontsize=30)\n",
    "    plt.title('0-pass,1-defect',fontsize=40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要去csv檔案寫上答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "correct = 0\n",
    "with open('result.csv', newline='') as csvfile:    \n",
    "    rows = csv.DictReader(csvfile)\n",
    "    for row in rows:\n",
    "        if row['label'] == row['answer']:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(row['id'] + \" 答案是:\" + row['answer'] + \" 誤判成: \" + row['label'])\n",
    "            \n",
    "print(\"--------------------------------------\")\n",
    "print(\"準確率 ： %.8f\" %(correct/(rows.line_num-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用model做驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "\n",
    "test_data=[]\n",
    "id=[]\n",
    "counter=0\n",
    "IMAGE_SIZE=80\n",
    "\n",
    "for file in os.listdir(\"..\\\\tensorflow-classification\\\\dataset\\\\test_pass\"):\n",
    "    \n",
    "    pic = \"..\\\\tensorflow-classification\\\\dataset\\\\test_pass\\\\\" + file\n",
    "    image_data=cv2.imread(os.path.join(pic), cv2.IMREAD_COLOR)\n",
    "    \n",
    "    try:\n",
    "        image_data=cv2.resize(image_data,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "        test_data.append(image_data/255)\n",
    "        id.append((file.split(\".\"))[0])\n",
    "    except:\n",
    "        print (\"WHAT THE FUCK\")\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "test_data=np.array(test_data)\n",
    "print (test_data.shape)\n",
    "test_data=test_data.reshape((test_data.shape)[0],(test_data.shape)[1],(test_data.shape)[2],3)\n",
    "dataframe_output=pd.DataFrame({\"id\":id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2_1:0\", shape=(?, 80, 80, 3), dtype=float32)\n",
      "Tensor(\"out_1/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = load_model('model01.h5')\n",
    "print(model.input)\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels=model.predict(test_data)\n",
    "predicted_labels=np.round(predicted_labels,decimals=2)\n",
    "labels=[1 if value>0.5 else 0 for value in predicted_labels]\n",
    "\n",
    "dataframe_output[\"label\"]=labels\n",
    "print(dataframe_output)\n",
    "dataframe_output.to_csv('result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "with open('result.csv', newline='') as csvfile:    \n",
    "    rows = csv.DictReader(csvfile)\n",
    "    for row in rows:\n",
    "        if row['label'] == row['answer']:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(row['id'] + \" 答案是:\" + row['answer'] + \" 誤判成: \" + row['label'])\n",
    "            \n",
    "print(\"--------------------------------------\")\n",
    "print(\"準確率 ： %.8f\" %(correct/(rows.line_num-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
